{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f2a314",
   "metadata": {},
   "source": [
    "# SparkSession\n",
    "\n",
    "U uvodnom tekstu smo ukratko spomenuli Spark ulaznu točku zvanu SparkContext koja predstavlja konekciju sa Spark klasterom - SparkSession je nadskup toga. Okružuje SparkContext i pruža mogućnost interakcije sa Spark SQL API-jem koji sadrži DF koji ćemo koristiti u većini naših programa. \n",
    "\n",
    "Prije pokretanja PySpark koda potrebno je kreirati SparkSession instancu. Dok je pokrenuta možete dohvatiti pregled vašeg Spark klastera i svih operacija na http://localhost:4040.\n",
    "\n",
    "SparkSession objektu ćete dodijeliti određene parametre kako biste postavili i konfigurirali Spark aplikaciju - neki od parametara uključuju naziv aplikacije, URL i postavke alokacije memorije.Tako naša aplikacija postavlja sljedeće parametre:\n",
    "* `builder` - objekt za kreiranje i konfiguraciju SparkSessiona\n",
    "* `master(\"local[*]\")` - lokalno izvršavanje aplikacije gdje * označava izrvšavanje aplikacije na svim dostupnim jezgrama - ovdje možete upisati i broj jezgri\n",
    "* `appName(\"SparkVjezba\")` - ime aplikacije\n",
    "* `getOrCreate()` - kreira novi SparkSession ako ne postoji ili vraća postojeći ako se već izvodi te time osigurava da je samo jedan SparkSession objekt kreiran za aplikaciju što je važno za efikasnu upotrebu resursa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90163a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\python311\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\python311\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a4fd87",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      4\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkVjezba\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0e7a1",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "Ako ste korisitli Pandas biblioteku u Pythonu vjerojatno ste upoznati s konceptom DataFramea. Ipak, važno je naglasiti da DataFrame u Pandasu i Sparku nisu isti; neke od razlika su:\n",
    "| Aspekt | Pandas DataFrame | Spark DataFrame |\n",
    "|:--------:|:-----------------:|:----------------:|\n",
    "| Izvršavanje | Manji setovi podataka i izvršavanje na jednom stroju | Veliki setovi podataka i izvršavanje na više storjeva |\n",
    "| Evaluacija | Agilna | Lijena |\n",
    "| Brzina | Brži za manje skupove podataka| Brži za veće skupove podataka |\n",
    "| Izvori podataka | Zahtijeva dodatne bibioloteke za čitanje i pisanje raznih izvora podataka | Ugrađena potpora za čitanje i pisanje raznih izvora podataka| |\n",
    "\n",
    "<br/>\n",
    "Dataframe u Sparku možete shvatiti kao tablicu s redovima i stupcima koja sadrži veliku količinu podataka. Obrada podataka u Dataframeu se vrši korištenjem transformacija i akcija, a rezultat se može spremiti u novi Dataframe. Dataframe je također vrlo fleksibilan i može se koristiti za razne obrade podataka kao što su spajanje, filtriranje, sortiranje, agregacija i grupiranje podataka. \n",
    "\n",
    "## Kreiranje i korištenje DataFramea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8d7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kreacija DataFramea\n",
    "data = [(1,2), (3,4)]\n",
    "schema = \"neparni int, parni int\"\n",
    "df_list = spark.createDataFrame(data, schema)\n",
    "\n",
    "# prikaz DataFramea\n",
    "df_list.show()\n",
    "\n",
    "# prikaz scheme DataFramea\n",
    "df_list.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selekcija stupaca\n",
    "df_list.select(['neparni']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodavanje stupaca\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_list.withColumn(\"zbroj\", col(\"neparni\") + col(\"parni\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9daef",
   "metadata": {},
   "source": [
    "Prilikom stvaranja DF-a ukoliko nema zaglavlja (ili se ona ne čitaju) zadana vrijednost ukoliko se DF stvara iz CSV datoteke je _cn - gdje n predstavlja broj stupca počevši od nule, dok ukoliko se stvara iz liste zadana vrijednost je _n - gdje je n ekvivalent prethodnom n-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023835cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preimenovanje stupaca\n",
    "df_list_n = spark.createDataFrame(data)\n",
    "df_list_n.withColumnRenamed('_1', 'neparni') \\\n",
    "         .withColumnRenamed('_2', 'parni') \\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca4311",
   "metadata": {},
   "source": [
    "## Spark SQL i Window funkcije\n",
    "\n",
    "Windowing u Sparku omogućuje obradu podataka u prozorima; odnosno, funkcije se mogu primijeniti i na više elemenata, a ne samo na pojedinačni element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_data = ((\"John\", \"Engineering\", 7000),\n",
    "                 (\"Jane\", \"Marketing\", 5000),\n",
    "                 (\"Bob\", \"Engineering\", 6000),\n",
    "                 (\"Mary\", \"Sales\", 5500),\n",
    "                 (\"Alex\", \"Marketing\", 4500),\n",
    "                 (\"Mike\", \"Engineering\", 7500),\n",
    "                 (\"Julie\", \"Sales\", 6000),\n",
    "                 (\"Sam\", \"Finance\", 6500),\n",
    "                 (\"Lisa\", \"Marketing\", 4500),\n",
    "                 (\"Tom\", \"Engineering\", 8000))\n",
    "\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a3c692",
   "metadata": {},
   "source": [
    "### Funkcije rangiranja\n",
    "\n",
    "Funkcija row_number koristi se za dodjelu slijednog broja retka koji počinje od 1 - svaki broj koristi samo jednom neovisno o tome je li došlo do izjenačenja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e86947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_order = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df_employees.withColumn(\"row_number\", row_number().over(window_order)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220afb75",
   "metadata": {},
   "source": [
    "Funkcije rank i dense_rank dodjeljuju rang rezultatu unutar prozora. Dok rank ostavlja prazna mjesta ako dolazi do izjednačenja, dense_rank preskače praznine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4179a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "df_employees.withColumn(\"rank\", rank().over(window_order)) \\\n",
    "            .withColumn(\"dense_rank\", dense_rank().over(window_order)) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239217f4",
   "metadata": {},
   "source": [
    "Funkcija percent_rank namijenjena je za izračunavanje percentila za svaki redak unutar prozora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75beac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "df_employees.withColumn(\"percent_rank\", percent_rank().over(window_order)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c066492",
   "metadata": {},
   "source": [
    "Funkcija prozora ntile koristi se za podjelu rezultata unutar prozora u n jednakih dijelova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fa23c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "df_employees.withColumn(\"ntile\", ntile(2).over(window_order)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaebdb3",
   "metadata": {},
   "source": [
    "### Analitičke funkcije \n",
    "\n",
    "Funkcija cume_dist koristi se za izračun kumulativne distribucije vrijednosti unutar svakog prozora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd03ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "\n",
    "df_employees.withColumn(\"cume_dist\", cume_dist().over(window_order)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ec7e0",
   "metadata": {},
   "source": [
    "Funkcija lag se koristi za dobivanje n-tog prethodnika reda unutar prozora, dok ćete funkciju lead iskoristiti kada želite dobit n-tog sljedbenika reda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "df_employees.withColumn(\"lag\", lag(\"salary\", 1).over(window_order)) \\\n",
    "            .withColumn(\"lead\", lead(\"salary\", 1).over(window_order)) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48187f15",
   "metadata": {},
   "source": [
    "### Agregatne funkcije\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12766770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, sum, min, max\n",
    "\n",
    "window_aggregation  = Window.partitionBy(\"department\")\n",
    "\n",
    "df_employees.withColumn(\"row\", row_number().over(window_order)) \\\n",
    "            .withColumn(\"avg\", avg(col(\"salary\")).over(window_aggregation)) \\\n",
    "            .withColumn(\"sum\", sum(col(\"salary\")).over(window_aggregation)) \\\n",
    "            .withColumn(\"min\", min(col(\"salary\")).over(window_aggregation)) \\\n",
    "            .withColumn(\"max\", max(col(\"salary\")).over(window_aggregation)) \\\n",
    "            .where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\") \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa89bae",
   "metadata": {},
   "source": [
    "## Čitanje iz datoteke\n",
    "PySpark posjeduje kapacitet otkrivanja scheme - možete uključiti tu opciju postavljajući inferSchema na True. Ovaj opcionalni parametar tjera PySpark da prođe kroz podatke dvaput - prvi put kako bi postavio tip svakog stupca i drugi put da bi učitao podatke. Time učitavanje traje duže, ali nam pomaže izbjeći ručno pisanje scheme. Preuzmite skup podataka sa sljedeće poveznice https://www.kaggle.com/datasets/crawford/80-cereals te ga stavite u mapu u kojoj se nalazi vaša bilježnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33415178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Učitavanje datoteke sa zaglavljem i otkrivanjem scheme\n",
    "df_cereal = spark.read.format('csv').option('header', True).option('inferSchema', True).load('cereal.csv')\n",
    "\n",
    "# Prikaz samo prva tri reda\n",
    "df_cereal.show(3)\n",
    "\n",
    "# Primijetite tipove \n",
    "df_cereal.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a74ecc",
   "metadata": {},
   "source": [
    "## Agregacije nad grupiranim stupcem\n",
    "Ako želite grupirati stupce možete se poslužiti transformacijom `groupby('imestupca')`. Metoda `agg()` (agg iz \"aggregation\") prihvaća jednu ili više agregatnih funkcija iz modula `pyspark.sql.functions`. U primjeru ispod računamo prosječni broj kalorija po tipu žitarica te novi stupac imenujemo metodom `alias('imestupca')` koji bi u protivnom nosio naziv prema funkcijama izvedenim u `agg()` metodi - u konkretnom slučaju \"round(avg(calories), 2)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df_cereal.groupBy('type') \\\n",
    "            .agg(round(avg('calories'), 2).alias('avg_calories')) \\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bd2cc",
   "metadata": {},
   "source": [
    "## Važno! \n",
    "Prije nastavka rješavanja obavite zadatak iz poglavlja \"*Povezivanje PySparka s Kafkom*\". Obratite pozornost na stupce u datoteci - ako ste čitali cijelu vrijednost cijeli redak će vam biti zapisan pod jedan stupac. Razdvojite datoteku po stupcima prije nastavka - npr. spremanjem datoteke nanovo u CSV formatu.\n",
    "## Dnevni prosjek \n",
    "Vaš zadatak je prikazati kretanje prosječne dnevne cijene kroz proteklih sto dana koristeći DataFrame kao strukturu podataka. Ispišite vaše rješenje na ekran - poredajte elemente silazno po prosječnoj cijeni zaokruženoj na jednu decimalu te im dodijelite rang. Možete koristiti `from_unixtime` iz `pyspark.sql.functions` modula za izvršenje zadatka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prostor za rješavanje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f0999",
   "metadata": {},
   "source": [
    "# Resilient Distributed Dataset (RDD) \n",
    "Postoje dva načina kako kreirati RDD: \n",
    "* paralelizacijom postojeće kolekcije - pozivanjem metode parallelize\n",
    "* referenciranjem skupa podataka u vanjsokom sustavu za pohranu - pozivanjem metode textfile koja kao argument prihvaća put do datoteke\n",
    "\n",
    "\n",
    "RDD se poziva nad SparkContextom koji možete izvući iz SparkSession objekta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext objekt iz SparkSessiona\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Paralelizacija postojeće kolekcije\n",
    "rdd_array = sc.parallelize([1, 2, 3])\n",
    "\n",
    "# Učitavanje iz datoteke - zamijenite naziv CSV datoteke sa svojim\n",
    "rdd_csv = sc.textFile(\"coincap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c446b93",
   "metadata": {},
   "source": [
    "## Osnovne akcije nad RDD-ovima \n",
    "Metoda `collect()` dohvaća cijeli RDD (primijetite kako je svaki redak pročitan kao jedan element iako sadrži dva podatka). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42657b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_csv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a844d0",
   "metadata": {},
   "source": [
    "Metoda `count()` pokazuje broj elemenata u RDD-u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_csv.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519b29d",
   "metadata": {},
   "source": [
    "Kako biste uzeli određeni broj elemenata (po redu) potrebno je pozvati `take(n)` gdje n ukazuje koji je to broj elemenata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f329b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99354675",
   "metadata": {},
   "source": [
    "## Transformacije nad RDD-ovima\n",
    "\n",
    "Tranformacija filter se koristi kako bi se zadržali elementi koji ispunjavaju određeni uvjet; nad svakim elementom se poziva funkcija te ako je odgovor na tu funkciju True taj element se zadržava."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb46de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "veci_od_jedan = rdd_array.filter(lambda x: x > 1)\n",
    "veci_od_jedan.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e91dc",
   "metadata": {},
   "source": [
    "Koristeći map transformaciju možete nad svakim elementom RDD-a izvesti određenu funkciju. Primjerice, ako imate csv datoteku u kojoj imate više stupaca - te elemente možete pročitati pozivajući funkciju koja će razdvojiti vaš redak u elemente po delimiteru u vašoj datoteci.\n",
    "\n",
    "Kao vježbu napišite naredbu s lambda funkcijom koja će vaš rdd_csv razdvojiti po stupcima te prikažite novi RDD na zaslonu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prostor za rješavanje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d7190",
   "metadata": {},
   "source": [
    "Transformacija reduce primjenjuje danu funkciju na dva parametra istog tipa te vraća jedan rezultat. Ta funkcija se primjenjuje elementima u strukturi sličnoj stablu, gdje se na svakoj razini broj elemenata prepolavlja sve dok se ne dođe do jednog elementa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae80a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = rdd_array.reduce(lambda x, y: x + y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2aa3e",
   "metadata": {},
   "source": [
    "Transformacija groupByKey grupira zapise prema ključu i vraća RDD gdje svaki zapis ima ključ i listu vrijednosti povezanih s tim ključem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76582e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_employees = sc.parallelize([(\"Engineering\", \"Mike\"), (\"Engineering\", \"Tom\"), (\"Marketing\", \"Lisa\")])\n",
    "\n",
    "for key, values in rdd_employees.groupByKey().collect():\n",
    "    print(f\"{key}: {list(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa7029",
   "metadata": {},
   "source": [
    "## Najviše dnevne cijene\n",
    "\n",
    "Vaš zadatak je pronaći najveći dnevni maksimum kroz protekla dva tjedna koristeći RDD kao strukturu podataka. Pripazite na tipove učitanih podataka. Ovisno o kodu koji unesete možete nepovratno \"pokvariti\" kernel kada računate s RDD-om u Jupyteru pa ga stoga iznova pokrenite s vremena na vrijeme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e335c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prostor za rješavanje"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
